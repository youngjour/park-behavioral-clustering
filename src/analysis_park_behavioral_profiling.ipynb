{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d39e5e0",
   "metadata": {},
   "source": [
    "# Park Behavioral Profiling & Structural Diagnosis\n",
    "\n",
    "This notebook reproduces the analysis for:\n",
    "1. Behavioral Clustering (K-Means, k=5)\n",
    "2. Predictive Modeling (LOOCV)\n",
    "3. Model Interpretation (SHAP, PDP)\n",
    "4. Structural Diagnosis (Efficiency Index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d8e169",
   "metadata": {},
   "source": [
    "## 1. Behavioral Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349b1802",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Set paths\n",
    "BASE_DIR = r\"c:\\Users\\jour\\Documents\\GitHub\\accessibility_park-1\\park-behavioral-clustering\"\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\", \"processed\")\n",
    "RESULTS_TABLES_DIR = os.path.join(BASE_DIR, \"results\", \"tables\")\n",
    "RESULTS_FIGURES_DIR = os.path.join(BASE_DIR, \"results\", \"figures\")\n",
    "SRC_DIR = os.path.join(BASE_DIR, \"src\")\n",
    "\n",
    "os.makedirs(RESULTS_TABLES_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_FIGURES_DIR, exist_ok=True)\n",
    "os.makedirs(SRC_DIR, exist_ok=True)\n",
    "\n",
    "# 1. Load Data\n",
    "clusters_path = os.path.join(DATA_DIR, \"enhanced_behavioral_clusters.csv\")\n",
    "features_path = os.path.join(RESULTS_TABLES_DIR, \"Table_S1_Clustering_Features.csv\")\n",
    "\n",
    "df = pd.read_csv(clusters_path)\n",
    "features_df = pd.read_csv(features_path)\n",
    "\n",
    "# Extract feature names\n",
    "feature_names = features_df['Feature Name'].tolist()\n",
    "print(f\"Loaded {len(feature_names)} features for clustering.\")\n",
    "\n",
    "# 2. Preprocess\n",
    "X = df[feature_names]\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 3. K-Means Clustering (k=5)\n",
    "k = 5\n",
    "kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# 4. Evaluation Metrics & Plot\n",
    "# Elbow & Silhouette\n",
    "inertias = []\n",
    "silhouettes = []\n",
    "k_range = range(2, 10)\n",
    "\n",
    "for n_k in k_range:\n",
    "    km = KMeans(n_clusters=n_k, random_state=42, n_init=10)\n",
    "    km_labels = km.fit_predict(X_scaled)\n",
    "    inertias.append(km.inertia_)\n",
    "    silhouettes.append(silhouette_score(X_scaled, km_labels))\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Number of clusters (k)')\n",
    "ax1.set_ylabel('Inertia', color=color)\n",
    "ax1.plot(k_range, inertias, marker='o', color=color, label='Inertia')\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('Silhouette Score', color=color)\n",
    "ax2.plot(k_range, silhouettes, marker='s', color=color, label='Silhouette')\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "plt.title('K-Means Optimization: Elbow Method & Silhouette Score')\n",
    "fig.tight_layout()\n",
    "plt.savefig(os.path.join(RESULTS_FIGURES_DIR, 'Figure_1_Clustering_Optimization.png'))\n",
    "plt.close()\n",
    "\n",
    "# 5. Update Dataset\n",
    "df['cluster'] = labels\n",
    "\n",
    "# Save updated dataset\n",
    "df.to_csv(clusters_path, index=False)\n",
    "print(f\"Updated clusters saved to {clusters_path}\")\n",
    "\n",
    "# 6. Generate Cluster Description Table\n",
    "# Calculate mean of features per cluster\n",
    "cluster_summary = df.groupby('cluster')[feature_names].mean().T\n",
    "cluster_summary.columns = [f'Cluster {i}' for i in range(k)]\n",
    "cluster_summary.to_csv(os.path.join(RESULTS_TABLES_DIR, 'Table_4_Cluster_Descriptions.csv'))\n",
    "print(\"Cluster description table saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109b1771",
   "metadata": {},
   "source": [
    "## 2. Predictive Modeling (LOOCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b61872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import os\n",
    "\n",
    "# Set paths\n",
    "BASE_DIR = r\"c:\\Users\\jour\\Documents\\GitHub\\accessibility_park-1\\park-behavioral-clustering\"\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\", \"processed\")\n",
    "RESULTS_TABLES_DIR = os.path.join(BASE_DIR, \"results\", \"tables\")\n",
    "SRC_DIR = os.path.join(BASE_DIR, \"src\")\n",
    "\n",
    "# 1. Load Data\n",
    "clusters_path = os.path.join(DATA_DIR, \"enhanced_behavioral_clusters.csv\")\n",
    "dataset_path = os.path.join(DATA_DIR, \"final_dataset_18parks.csv\")\n",
    "features_path = os.path.join(RESULTS_TABLES_DIR, \"Table_S2_Predictive_Features.csv\")\n",
    "\n",
    "df_clusters = pd.read_csv(clusters_path)\n",
    "df_data = pd.read_csv(dataset_path)\n",
    "df_features = pd.read_csv(features_path)\n",
    "\n",
    "# Merge: enhanced_clusters has 'cluster', final_dataset has 'park_name' and features\n",
    "# Note: final_dataset ALSO has a 'cluster' column which might be old. We should drop it or use the one from enhanced_clusters.\n",
    "# enhanced_clusters has 'park_name' as index? No, let's check. \n",
    "# Step 14 output shows 'park_name' is NOT a column, it seems to be in the first column but unnamed in header?\n",
    "# Wait, look at Step 14 output: \"Gangseo Hangang Park\" is the first value. The header line is:\n",
    "# \",avg_ppltn,max_ppltn...\" -> The first column name is empty string or space.\n",
    "# I need to handle this.\n",
    "\n",
    "# Reload with specific handling\n",
    "df_clusters = pd.read_csv(clusters_path)\n",
    "if df_clusters.columns[0].strip() == '' or 'Unnamed' in df_clusters.columns[0]:\n",
    "    df_clusters.rename(columns={df_clusters.columns[0]: 'park_name'}, inplace=True)\n",
    "\n",
    "# Dataset 18 parks might be fine.\n",
    "df_data = pd.read_csv(dataset_path)\n",
    "\n",
    "# Merge\n",
    "# We want 'cluster' from df_clusters and FEATURES from df_data.\n",
    "merged_df = pd.merge(df_clusters[['park_name', 'cluster']], df_data, on='park_name', suffixes=('_target', '_data'))\n",
    "\n",
    "# Use the cluster from df_clusters as target (it deals with the re-run k=5)\n",
    "target_col = 'cluster_target'\n",
    "\n",
    "# Get Predictive Features\n",
    "feature_list = df_features['Feature Name'].tolist()\n",
    "# Filter out any that might be missing\n",
    "available_features = [f for f in feature_list if f in merged_df.columns]\n",
    "missing_features = [f for f in feature_list if f not in merged_df.columns]\n",
    "if missing_features:\n",
    "    print(f\"Warning: Missing features from dataset: {missing_features}\")\n",
    "\n",
    "X = merged_df[available_features]\n",
    "y = merged_df[target_col]\n",
    "park_names = merged_df['park_name']\n",
    "\n",
    "print(f\"Data Loaded. Shape: {X.shape}. Target distribution:\\n{y.value_counts()}\")\n",
    "\n",
    "# 2. Predictive Modeling with LOOCV\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=2000, random_state=42),\n",
    "    'SVM': SVC(kernel='rbf', probability=True, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
    "}\n",
    "\n",
    "results = []\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "print(\"\\nStarting LOOCV...\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    y_true_all = []\n",
    "    y_pred_all = []\n",
    "    \n",
    "    for train_index, test_index in loo.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        if name == 'XGBoost':\n",
    "            # XGBoost requires 0..N-1 labels. \n",
    "            # If a class is missing in y_train, we need to remap.\n",
    "            from sklearn.preprocessing import LabelEncoder\n",
    "            le = LabelEncoder()\n",
    "            y_train_enc = le.fit_transform(y_train)\n",
    "            \n",
    "            # Check if we have enough classes\n",
    "            if len(np.unique(y_train_enc)) < 2:\n",
    "                 pred = [y_train.iloc[0]] # Fallback\n",
    "            else:\n",
    "                model.fit(X_train, y_train_enc)\n",
    "                pred_enc = model.predict(X_test)\n",
    "                # Map back. Note: if model predicts a class, it's an index in le.classes_\n",
    "                # But wait, if X_test leads to a prediction, it gives an integer.\n",
    "                # We need to inverse transform.\n",
    "                pred = le.inverse_transform(pred_enc)\n",
    "        \n",
    "        else:\n",
    "            # Standard Sklearn models\n",
    "            if len(y_train.unique()) < 2:\n",
    "                pred = [y_train.iloc[0]] \n",
    "            else:\n",
    "                model.fit(X_train, y_train)\n",
    "                pred = model.predict(X_test)\n",
    "        \n",
    "        y_true_all.append(y_test.values[0])\n",
    "        y_pred_all.append(pred[0])\n",
    "        \n",
    "    acc = accuracy_score(y_true_all, y_pred_all)\n",
    "    f1 = f1_score(y_true_all, y_pred_all, average='macro')\n",
    "    \n",
    "    print(f\"Model: {name} | Accuracy: {acc:.4f} | F1-Macro: {f1:.4f}\")\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': acc,\n",
    "        'F1-Score (Macro)': f1\n",
    "    })\n",
    "\n",
    "# 3. Save Results\n",
    "results_df = pd.DataFrame(results)\n",
    "output_path = os.path.join(RESULTS_TABLES_DIR, \"Table_6_Model_Performance.csv\")\n",
    "results_df.to_csv(output_path, index=False)\n",
    "print(f\"\\nResults saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9d8b28",
   "metadata": {},
   "source": [
    "## 3. Interpretation & Diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97600bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "import os\n",
    "\n",
    "# Set paths\n",
    "BASE_DIR = r\"c:\\Users\\jour\\Documents\\GitHub\\accessibility_park-1\\park-behavioral-clustering\"\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\", \"processed\")\n",
    "RESULTS_TABLES_DIR = os.path.join(BASE_DIR, \"results\", \"tables\")\n",
    "RESULTS_FIGURES_DIR = os.path.join(BASE_DIR, \"results\", \"figures\")\n",
    "\n",
    "# 1. Load Data\n",
    "clusters_path = os.path.join(DATA_DIR, \"enhanced_behavioral_clusters.csv\")\n",
    "dataset_path = os.path.join(DATA_DIR, \"final_dataset_18parks.csv\")\n",
    "features_path = os.path.join(RESULTS_TABLES_DIR, \"Table_S2_Predictive_Features.csv\")\n",
    "\n",
    "df_clusters = pd.read_csv(clusters_path)\n",
    "if df_clusters.columns[0].strip() == '' or 'Unnamed' in df_clusters.columns[0]:\n",
    "    df_clusters.rename(columns={df_clusters.columns[0]: 'park_name'}, inplace=True)\n",
    "\n",
    "df_data = pd.read_csv(dataset_path)\n",
    "df_features = pd.read_csv(features_path)\n",
    "\n",
    "# Merge\n",
    "# Need 'avg_ppltn' for Volume target (Efficiency Index)\n",
    "merged_df = pd.merge(df_clusters[['park_name', 'cluster', 'avg_ppltn']], df_data, on='park_name', suffixes=('_target', '_data'))\n",
    "\n",
    "feature_list = df_features['Feature Name'].tolist()\n",
    "available_features = [f for f in feature_list if f in merged_df.columns]\n",
    "\n",
    "X = merged_df[available_features]\n",
    "y_cls = merged_df['cluster_target']\n",
    "y_vol = np.log1p(merged_df['avg_ppltn']) # Target for regression\n",
    "\n",
    "print(f\"Data Loaded. X Shape: {X.shape}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Part 1: Interpretation (XGBoost Classifier)\n",
    "# ---------------------------------------------------------\n",
    "print(\"Running Interpretation (SHAP & PDP)...\")\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y_cls_enc = le.fit_transform(y_cls)\n",
    "\n",
    "# Train Classifier\n",
    "model_cls = xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
    "model_cls.fit(X, y_cls_enc)\n",
    "\n",
    "# SHAP\n",
    "# Explainer\n",
    "explainer = shap.TreeExplainer(model_cls)\n",
    "shap_values = explainer.shap_values(X)\n",
    "\n",
    "# Summary Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "# For multiclass, shap_values is a list of arrays (one for each class).\n",
    "# Summary plot for multiclass sums them up or we can pick one?\n",
    "# Usually 'shap.summary_plot(shap_values, X)' handles multiclass by stacking or via color.\n",
    "shap.summary_plot(shap_values, X, show=False)\n",
    "plt.savefig(os.path.join(RESULTS_FIGURES_DIR, 'Figure_2_SHAP_Summary.png'), bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# PDP\n",
    "# Get Top 3 features by importance\n",
    "importances = model_cls.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "top_features = X.columns[indices[:3]].tolist()\n",
    "print(f\"Top features for PDP: {top_features}\")\n",
    "\n",
    "# Create PDP\n",
    "# Note: multiclass PDP usually requires specifying the target class. \n",
    "# We'll plot for each class or just average?\n",
    "# Sklearn's PartialDependenceDisplay allows 'target' param. \n",
    "# We'll generate PDP for class 0 (Evening Urban) as example, or iterate?\n",
    "# Let's plot for the most populous class or just default (which might average for binary, or require target for multi).\n",
    "# For XGBoost estimator in sklearn wrapper, we can use sklearn's display.\n",
    "# We will generate one figure with subplots for the top features.\n",
    "# We have to pick a target class for multiclass PDP. Let's pick all classes in one plot per feature? Too messy.\n",
    "# Let's pick the 'Evening Urban' (Cluster 0) and 'Mega' (Cluster 3 or so). \n",
    "# Actually, let's just do it for Class 0 for now as an example, or loop through top features.\n",
    "# If we don't specify target for multiclass, it might fail.\n",
    "# Let's assume Class 0 is interesting.\n",
    "target_class_idx = 0 \n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "PartialDependenceDisplay.from_estimator(model_cls, X, top_features, target=target_class_idx, ax=ax)\n",
    "plt.suptitle(f\"Partial Dependence Plots (Target: Class {le.inverse_transform([target_class_idx])[0]})\")\n",
    "plt.savefig(os.path.join(RESULTS_FIGURES_DIR, 'Figure_3_PDP.png'), bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Part 2: Diagnosis (XGBoost Regressor)\n",
    "# ---------------------------------------------------------\n",
    "print(\"Running Structural Diagnosis (Efficiency Index)...\")\n",
    "\n",
    "model_reg = xgb.XGBRegressor(random_state=42)\n",
    "model_reg.fit(X, y_vol)\n",
    "y_pred_vol = model_reg.predict(X)\n",
    "\n",
    "# Calculate Efficiency\n",
    "residuals = y_vol - y_pred_vol\n",
    "# Efficiency Index: Let's use Residual directly. Positive = More efficient than expected.\n",
    "\n",
    "merged_df['log_actual'] = y_vol\n",
    "merged_df['log_predicted'] = y_pred_vol\n",
    "merged_df['efficiency_residual'] = residuals\n",
    "\n",
    "# Define Outliers\n",
    "mean_res = residuals.mean()\n",
    "std_res = residuals.std()\n",
    "threshold = 1.0 * std_res # Using 1.0 std for small sample to highlight SOME outliers\n",
    "\n",
    "merged_df['efficiency_status'] = 'Normal'\n",
    "merged_df.loc[merged_df['efficiency_residual'] > mean_res + threshold, 'efficiency_status'] = 'Positive Outlier'\n",
    "merged_df.loc[merged_df['efficiency_residual'] < mean_res - threshold, 'efficiency_status'] = 'Negative Outlier'\n",
    "\n",
    "# Save Table\n",
    "out_table = merged_df[['park_name', 'cluster_target', 'avg_ppltn', 'log_actual', 'log_predicted', 'efficiency_residual', 'efficiency_status']]\n",
    "out_table.to_csv(os.path.join(RESULTS_TABLES_DIR, 'Table_Efficiency_Diagnosis.csv'), index=False)\n",
    "\n",
    "# Plot Diagnosis\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(data=merged_df, x='log_predicted', y='log_actual', hue='efficiency_status', style='cluster_target', s=100)\n",
    "# Add diagonal line\n",
    "min_val = min(y_vol.min(), y_pred_vol.min())\n",
    "max_val = max(y_vol.max(), y_pred_vol.max())\n",
    "plt.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.5)\n",
    "\n",
    "# Label outliers\n",
    "for i, row in merged_df.iterrows():\n",
    "    if row['efficiency_status'] != 'Normal':\n",
    "        plt.text(row['log_predicted'], row['log_actual'], row['park_name'], fontsize=9)\n",
    "\n",
    "plt.title('Structural Diagnosis: Actual vs Predicted Volume')\n",
    "plt.xlabel('Predicted Log Volume (Potential)')\n",
    "plt.ylabel('Actual Log Volume (Usage)')\n",
    "plt.savefig(os.path.join(RESULTS_FIGURES_DIR, 'Figure_4_Efficiency_Diagnosis.png'))\n",
    "plt.close()\n",
    "\n",
    "print(\"Interpretation and Diagnosis Complete.\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
